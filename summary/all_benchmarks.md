| Model               |Model Type| IFEval | MMLU | CMMLU | C-Eval | GPQA | GSM8K | MATH | HumanEval | AlignBench | NaturalCodeBench |
|:--------------------|:--------:|:------:|:----:|:-----:|:------:|:----:|:-----:|:----:|:---------:|:----------:|:----------------:|
| Llama-3-8B          |Base      |        | 66.6 |       |  51.2  |      | 45.8  |      |   33.5    |            |                  |
| Llama-3-8B-Instruct |Chat      |  68.6  | 68.4 |       |  51.3  | 34.2 | 79.6  | 30.0 |   62.2    |    6.40    |       24.7       |
| ChatGLM3-6B-Base    |Base      |        | 61.4 |       |  69.0  | 26.8 | 72.3  | 25.7 |   58.5    |            |                  |
| ChatGLM3-6B-Chat    |Chat      |  28.1  | 61.4 |       |  69.0  | 26.8 | 72.3  | 25.7 |   58.5    |    5.18    |       11.3       |
| GLM-4-9B            |Base      |        | 74.7 |       |  77.1  | 34.3 | 84.0  | 30.4 |   70.1    |            |                  |
| GLM-4-9B-Chat       |Chat      |  69.0  | 74.7 |       |  77.1  | 34.3 | 84.0  | 30.4 |   70.1    |    7.01    |       32.2       |
| Yuan2.0-M32         |MoE(Chat) |        | 72.2 |       |        |      | 92.7  | 55.9 |   74.4    |            |                  |
| DeepSeek-V2         |MoE(Base) |        | 78.5 |       |  81.7  |      | 79.2  | 43.6 |   48.8    |            |                  |
| DeepSeek-V2(RL)     |MoE(Chat) |  63.8  | 77.8 |       |  78.0  |      | 92.2  | 53.9 |   81.1    |            |                  |
| DeepSeek-V2(SFT)    |MoE(Chat) |  64.1  | 78.4 |       |  80.9  |      | 90.8  | 52.7 |   76.8    |            |                  |
| Qwen2-72B           |Base      |        | 84.2 |  90.1 |  91.0  | 37.9 | 89.5  | 51.1 |   64.6    |            |                  |
| Qwen2-72B(Instruct) |Chat      |  77.6  | 82.3 |       |        | 42.2 | 91.1  | 59.7 |   86.0    |    8.27    |                  |
| Qwen2-7B(Instruct)  |Chat      |        | 70.5 |       |  77.2  |      | 82.3  | 49.6 |   79.9    |    7.27    |                  |